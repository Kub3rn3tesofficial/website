# Automated High Availability in kubeadm v1.15: Batteries Included But Swappable

[kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/) is a tool that enables Kubernetes administrators
to quickly and easily bootstrap minimum viable clusters that are fully compliant with 
[Certified Kubernetes](https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md) guidelines.
It’s been under active development by [SIG Cluster Lifecycle](https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle)
since 2016 and we graduated it from beta to
[stable and generally available (GA) in end of 2018](https://kubernetes.io/blog/2018/12/04 production-ready-kubernetes-cluster-creation-with-kubeadm/).

After this important milestone, the kubeadm team is now focused on the stability of the core feature set and working on
maturing existing features.

With this post, we are introducing the improvements made in the v1.15 release of kubeadm.

## The scope of kubeadm

kubeadm is focused on performing the actions necessary to get a minimum viable, secure cluster up and running in a
user-friendly way. kubeadm's scope is limited to the local machine’s filesystem and the Kubernetes API, and it is
intended to be a _composable building block for higher-level tools_. 

The core of the kubeadm interface is quite simple: new control plane nodes are created by running 
<strong><code>kubeadm init</code></strong>, worker nodes are joined to the control plane by running
<strong><code>kubeadm join</code></strong>. Also included are common utilities for managing already bootstrapped
clusters, such as control plane upgrades and token and certificate renewal.

To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of scope:

*   Infrastructure provisioning
*   Third-party networking
*   Non-critical add-ons for e.g. monitoring, logging, and visualization
*   Specific cloud provider integrations

Those tasks are addressed by other SIG Cluster Lifecycle projects, such as the
[Cluster API](https://github.com/kubernetes-sigs/cluster-api) for infrastructure provisioning and management. 

Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the
[control plane](https://kubernetes.io/docs/concepts/#kubernetes-control-plane). 


<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/kubeadm-v10.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/kubeadm-v10.png "image_tooltip")


## What’s new in kubeadm v1.15?

### High Availability to Beta

We are delighted to announce that automated support for High Availability clusters is graduating to **Beta** in kubeadm v1.15. Let’s give a great shout out to all the contributors that helped in this effort and to the early adopter users for the great feedback received so far!

But how does automated High Availability work in kubeadm?

The great news is that you can use the familiar `kubeadm init` or `kubeadm join` workflow for creating high availability cluster as well, with the only difference that you have to pass the `--control-plane` flag to `kubeadm join` when adding more control plane nodes. 

A 3-minute screencast of this feature is here:

[![asciicast](https://asciinema.org/a/252343.svg)](https://asciinema.org/a/252343)

In a nutshell:


1. **Before** you need an <span style="text-decoration:underline;">external load balancer</span>; providing the external load balancer is out of scope of kubeadm (HAproxy, Envoy, or a similar Load Balancer from a cloud provider work well)
2. **Then** <span style="text-decoration:underline;">kubeadm init</span> the cluster 

     \
On the first control plane node, create a configuration file called `kubeadm-config.yaml` and set the` controlPlaneEndpoint` field with the address or DNS and port of the load balancer.:


    ```
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterConfiguration
    ```

    `controlPlaneEndpoint: kubeadm-ha.luxas.dev:6443 \
`Initialize the control plane:

    ```
    sudo kubeadm init --config=kubeadm-config.yaml --upload-certs
    ```

    > The `--upload-certs` flags is used to upload the certificates that should be shared across all the control-plane instances to the cluster.  \
When the operation completes, the output is similar to this 

    ```
    You can now join any number of the control-plane node running the following command on each as root:                                                                     
     kubeadm join kubeadm-ha.luxas.dev:6443 --token ... \                                                                                               
       --discovery-token-ca-cert-hash sha256:... \                                                             
       --control-plane --certificate-key ...                                                      
    

    Please note that the certificate-key gives access to cluster sensitive data, keep it secret!                                                                             
    As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use                                                                                   
    "kubeadm init phase upload-certs --upload-certs" to reload certs afterward.                                                                                              
    

    Then you can join any number of worker nodes by running the following on each as root:                                                
    kubeadm join kubeadm-ha.luxas.dev:6443 --token ... \                                                                                                 
    ```

    `   --discovery-token-ca-cert-hash sha256:...` \


3. **After** <span style="text-decoration:underline;">kubeadm join** **</span>control-plane nodes and worker nodes in any order/at any time using the instruction printed above

For those interested in the details, there are many things that make this functionality possible. Most notably:



*   **Automated certificate transfer**. kubeadm implements an automatic certificate copy feature to automate the distribution of all the certificate authorities/keys that must be shared across all the control-planes nodes in order to get your cluster to work. This feature can be activated by passing  `--upload-certs` to  `kubeadm init`; see [configure and deploy an HA control plane](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/) for more details. This is an explicit opt-in feature, you can also distribute the certificates manually in your preferred way. \

*   **Dynamically-growing etcd cluster**. In case you are not providing an external etcd cluster, kubeadm automatically generates a static pod hosted etcd member on each control-plane node; all the etcd members are joined in a “stacked” etcd cluster that grows together with your high availability control-plane \

*   **Concurrent joining**. Similarly to what already implemented for worker nodes, it is possible to join control-plane nodes whenever, in any order; joining control-plane nodes in parallel is supported as well. \

*   **Upgradable**. The kubeadm upgrade workflow was improved in order to properly handle the HA scenario, and, after starting the upgrade with `kubeadm upgrade apply` as usual, users can now complete the upgrade process by using `kubeadm upgrade node` both on the remaining control-plane nodes and worker nodes

Finally, it is also worthy to notice that an entirely new test suite has been created specifically for ensuring High Availability in kubeadm will stay stable over time. 


### Certificate Management

Certificate management has become more simple and robust in kubeadm v1.15.

If you perform Kubernetes version upgrades regularly, kubeadm will now take care of keeping your cluster up to date and reasonably secure by [automatically rotating all your certificates](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal) at `kubeadm upgrade` time.

If instead, you prefer to renew your certificates manually, you can opt out from the automatic certificate renewal by passing `--certificate-renewal=false` to `kubeadm upgrade` commands. Then you can perform [manual certificate renewal](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal) with the `kubeadm alpha certs renew` command.


But there is more.

A new command `kubeadm alpha certs check-expiration` was introduced to allow users to 
[check certificate expiration](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#check-certificate-expiration). The output is similar to this:

```console
CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 May 15, 2020 13:03 UTC   364d            false
apiserver                  May 15, 2020 13:00 UTC   364d            false
apiserver-etcd-client      May 15, 2020 13:00 UTC   364d            false
apiserver-kubelet-client   May 15, 2020 13:00 UTC   364d            false
controller-manager.conf    May 15, 2020 13:03 UTC   364d            false
etcd-healthcheck-client    May 15, 2020 13:00 UTC   364d            false
etcd-peer                  May 15, 2020 13:00 UTC   364d            false
etcd-server                May 15, 2020 13:00 UTC   364d            false
front-proxy-client         May 15, 2020 13:00 UTC   364d            false
scheduler.conf             May 15, 2020 13:03 UTC   364d            false
```

You should expect also more work around certificate management in kubeadm in the next releases, with the introduction of ECDSA keys and with improved support for CA key rotation. Additionally, the commands staged under `kubeadm alpha` are expected to move top-level soon.


### Improved Configuration File Format

You can argue that there are hardly two Kubernetes clusters that are configured equally, and hence there is a need to customize how the cluster is set up depending on the environment. One way of configuring a component is via flags. However, this has some scalability limitations:


*   **Hard to maintain.** When $component’s flag set grows over 30+ flags, configuring it becomes really painful
*   **Complex upgrades**. When flags are removed, deprecated or changed, you need to upgrade of the binary at the same time as the arguments.
*   **Key-value limited**. There are simply many types of configuration you can’t express with the  `--key=value` syntax.
*   **Imperative**. In contrast to Kubernetes API objects themselves that are declaratively specified, flag arguments are imperative by design. 

This is a key problem for Kubernetes components in general, as some components have 150+ flags. With kubeadm we’re pioneering the ComponentConfig effort, and providing users with a small set of flags, but most importantly, a **declarative and versioned configuration file** for advance use-cases. We call this _ComponentConfig_. It has the following characteristics:


*   **Upgradable**: You can upgrade the binary, and still use the existing, older schema. Automatic migrations.
*   **Programmable**. Configuration expressed in JSON/YAML allows for consistent, and programmable manipulation
*   **Expressible**. Advanced patterns of configuration can be used and applied.
*   **Declarative**. OpenAPI information can easily be exposed / used for doc generation

In kubeadm v1.15, we have improved the structure and are releasing the new **v1beta2** format. Important to note that the existing **v1beta1** format released in v1.13 will still continue to work for several releases. This means you can upgrade kubeadm to v1.15, and still use your existing v1beta1 configuration files. When you’re ready to take advantage of the improvements made in v1beta2, you can perform an automatic schema migration using the `kubeadm config migrate` command. Important to note in this release is that support for the _v1alpha3_ version has been dropped.

During the course of the year, we’re looking forward to graduate the schema to General Availability `v1`.` If you’re interested in this effort, you can also join [WG Component Standard](https://github.com/kubernetes/community/tree/master/wg-component-standard).

## What’s next?

### 2019 plans

We are focusing our efforts this year around graduating the configuration file format to GA (`kubeadm.k8s.io/v1`)`, graduating this super-easy High Availability flow to stable, and providing better tools around rotating certificates needed for running the cluster automatically.

In addition to these three key milestones of our charter, we want to improve the following areas:

*   Support joining Windows nodes to a kubeadm cluster (with end-to-end tests)
*   Improve the upstream CI signal, mainly for HA and upgrades
*   Consolidate how Kubernetes artifacts are built and installed
*   Utilize Kustomize to allow for advanced, layered and declarative configuration

We make no guarantees that these deliverables will ship this year though, as this is a community effort. If you want to see these things happen, please join our SIG and start contributing! The ComponentConfig issues in particular need more attention.


### kubeadm now has a logo!

[Dan Kohn](https://github.com/dankohn) offered CNCF’s help with creating a logo for kubeadm in this cycle. [Alex Contini](https://github.com/alexcontini) created 19 (!) different logo options for the community to vote on. The public poll was active for around a week, and we got 386 answers. The winning option got 17.4% of the votes, so now it’s official!


<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/kubeadm-v11.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/kubeadm-v11.png "image_tooltip")


## Contributing

If this all sounds exciting, **join us**!

[SIG Cluster Lifecycle](https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle) has many different subprojects, where kubeadm is one of them. In the following picture you can see that there are many pieces in the puzzle, and we have a lot still to do.


<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/kubeadm-v12.png). Store image on your image server and adjust path/filename if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/kubeadm-v12.png "image_tooltip")


Some handy links if you want to start contribute:

*   We have a recorded [New Contributor Onboarding](https://www.youtube.com/watch?v=Bof9aveB3rA) session on YouTube.
*   Look out for “good first issue”, “help wanted” and “sig/cluster-lifecycle” labeled issues in our repositories 
    (e.g. [kubernetes/kubeadm](https://github.com/kubernetes/kubeadm))
*   Join **#sig-cluster-lifecycle**, **#kubeadm**, **#cluster-api**, **#minikube**, **#kind**, etc. in Slack
*   Join our public, bi-weekly SIG Cluster Lifecycle Zoom meeting at Tuesdays 9am PT
    *   Check out the [Meeting Notes](https://docs.google.com/document/d/1Gmc7LyCIL_148a9Tft7pdhdee0NBHdOfHS1SAF0duI4/edit) to join
*   Join our public, weekly kubeadm Office Hours Zoom meeting at Wednesdays 9am PT
    *   Check out the [Meeting Notes](https://docs.google.com/document/d/130_kiXjG7graFNSnIAgtMS1G8zPDwpkshgfRYS0nggo/edit) to join
*   Check out the [SIG Cluster Lifecycle Intro](https://youtu.be/bA2M41J4wvg) or the
    [kubeadm Deep Dive](https://youtu.be/spXSSIbZTqM) sessions from KubeCon Barcelona


### Thank You

This release wouldn’t have been possible without the help of the great people that have been contributing to SIG Cluster Lifecycle and kubeadm. We would like to thank all the kubeadm contributors and companies making it possible for their developers to work on Kubernetes!


## Written by:

Lucas Käldström, [@luxas](https://github.com/luxas)
kubeadm Subproject Owner and SIG Cluster Lifecycle co-chair
CNCF Ambassador
Kubernetes upstream contractor, last two years contracting for Weaveworks


Fabrizio Pandini, [@fabriziopandini](https://github.com/fabriziopandini)
kubeadm Subproject Owner, Independent
